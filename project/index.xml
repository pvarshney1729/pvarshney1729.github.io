<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Academic</title>
    <link>https://example.com/project/</link>
      <atom:link href="https://example.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 27 Apr 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu571820e5b5f211a19226089068501716_16981_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://example.com/project/</link>
    </image>
    
    <item>
      <title>Control Variates for Stochastic Gradient Hamiltonian Monte Carlo</title>
      <link>https://example.com/project/monte-carlo-/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/monte-carlo-/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Supervisor&lt;/strong&gt;: &lt;a href=&#34;https://dvats.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Dootika Vats&lt;/a&gt;, IIT Kanpur.&lt;/p&gt;
&lt;p&gt;We Explored the avenues of variance reduction methods such as Control Variates and their applications to Stochastic Gradient based Langevin Dynamics (SGLD), MCMC (SGMCMC) and Hamiltonian Monte Carlo (SGHMC) techniques. In this report, we mainly focused on
reproducing and extending the results of two papers: &amp;ldquo;Variance Reduction for Stochastic Gradient Optimisation&amp;rdquo; (Wang et. al. (2013)) and &amp;ldquo;Control Variates for Stochastic Gradient MCMC&amp;rdquo; (Baker et. al. (2019)) and explored their theoretical aspects. We extended the notion of Control Variates to different settings such as Metropolis-adjusted Langevin algorithm (MALA) explored how these ideas can be adopted and improved in these settings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speaker Diarization</title>
      <link>https://example.com/project/comparison-of-sgd-variants-for-stochastic-optimization/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/comparison-of-sgd-variants-for-stochastic-optimization/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Supervisor&lt;/strong&gt;: &lt;a href=&#34;https://home.iitk.ac.in/~vipular/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Vipul Arora&lt;/a&gt;, IIT Kanpur.&lt;/p&gt;
&lt;p&gt;In this term report we presented our model for the speaker diarization problem and explained how one can leverage Transfer Learning to quickly learn a model at the expense of negligible performance loss as compared to a fully trained one. Given the input utterances and their speaker identity labels, we extracted embeddings from short audio segments and used these embeddings to segregate the speaker segments within the input source. Building upon this model, we focused on transfer learning and manually adapting over various datasets so as to make our model more generic. We also focused on improving the DER along with experimenting with different embedding generation networks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing different SGD variants for online optimization</title>
      <link>https://example.com/project/speaker-diarization/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/speaker-diarization/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Supervisor&lt;/strong&gt;: &lt;a href=&#34;http://home.iitk.ac.in/~ketan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Ketan Rejawat&lt;/a&gt;, IIT Kanpur.&lt;/p&gt;
&lt;p&gt;In this term report we eproduced and extended the empirical results of “On the Insufficiency of Existing Momentum Schemes for Stochastic Optimization” and ”Accelerating Stochastic Gradient Descent For Least Squares Regression” by Kidambi et al. We showed experimentally that there exist simple stochastic problem instances where momentum based methods are sub-optimal and enjoy practical gains over SGD in deep learning applications due to minibatching and also established that ASGD and Adam can converge faster than all other methods irrespective of batch sizes&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
